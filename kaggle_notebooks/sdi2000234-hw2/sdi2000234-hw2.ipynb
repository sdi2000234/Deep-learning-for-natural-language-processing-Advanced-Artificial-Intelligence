{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96317,"databundleVersionId":11446837,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11479079,"sourceType":"datasetVersion","datasetId":7194598}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os, re, html, random, gc, time, math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\nfrom sklearn.metrics import (accuracy_score, classification_report,\n                             confusion_matrix, roc_curve, auc)\nfrom sklearn.model_selection import train_test_split, learning_curve\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nos.makedirs(\"figures\", exist_ok=True)   # ← create the folder if missing\n\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------------------------------------\n# 1. Load data -------------------------------------------------\nDATA_PATH = \"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2\"\ntrain_df = pd.read_csv(f\"{DATA_PATH}/train_dataset.csv\")\nval_df   = pd.read_csv(f\"{DATA_PATH}/val_dataset.csv\")\ntest_df  = pd.read_csv(f\"{DATA_PATH}/test_dataset.csv\")\nprint(\"Train / Val / Test:\", train_df.shape, val_df.shape, test_df.shape)\n\n# ---------------------------------------------------------\n# 2. Pre‑processing --------------------------------------------\nslang_dict = {\n    \"u\":\"you\",\"r\":\"are\",\"im\":\"i am\",\"ur\":\"your\",\"ure\":\"you are\",\"idk\":\"i dont know\",\n    \"brb\":\"be right back\",\"btw\":\"by the way\",\"lmk\":\"let me know\",\"tbh\":\"to be honest\",\n    \"ftw\":\"for the win\",\"fyi\":\"for your information\",\"diy\":\"do it yourself\",\n    \"gonna\":\"going to\",\"wanna\":\"want to\",\"da\":\"the\",\"omg\":\"oh my god\",\"gr8\":\"great\"\n}\nURL_RE  = re.compile(r\"http\\S+|www\\.\\S+\")\nMENTION = re.compile(r\"@\\w+\")\nELONG   = re.compile(r\"(.)\\1{2,}\")   # char repeated ≥3\nNON_ALPHANUM = re.compile(r\"[^a-z0-9\\s]\")\n\ndef preprocess(text:str)->list[str]:\n    text = html.unescape(text)\n    text = text.lower()\n    text = URL_RE.sub(\" \", text)\n    text = MENTION.sub(\" \", text)\n    text = text.replace(\"#\", \"\")            # keep hashtag word\n    # slang expansion\n    for slang, rep in slang_dict.items():\n        text = re.sub(rf\"\\b{slang}\\b\", rep, text)\n    # normalize elongations (sooo -> soo)\n    text = ELONG.sub(r\"\\1\\1\", text)\n    text = NON_ALPHANUM.sub(\" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    tokens = text.split()\n    return tokens if tokens else [\"<pad>\"]\n\n# Apply preprocessing\nfor df in (train_df, val_df, test_df):\n    df[\"tokens\"] = df[\"Text\"].apply(preprocess)\n\n# ---------------------------------------------------------\n# 3. Build vocabulary & load GloVe vectors ----------------\nEMBED_DIM = 200\nGLOVE_PATH = \"/kaggle/input/glove-twitter/glove.twitter.27B.200d.txt\"\n\n# Build vocab from train + val\nall_tokens = [tok for row in pd.concat([train_df[\"tokens\"],val_df[\"tokens\"]]) for tok in row]\nvocab_cnt  = Counter(all_tokens)\n# keep tokens with freq >=2  (OOV will be <unk>)\ntokens = [t for t, c in vocab_cnt.items() if c >= 2 and t != \"<pad>\"]\n\n# build an index sequence with NO gaps: 0=<pad>, 1=<unk>, 2…\nspecials = [\"<pad>\", \"<unk>\"]\nword2idx  = {tok: i for i, tok in enumerate(specials)}\n\nfor tok in tokens:                          # tokens never contains <pad>\n    word2idx[tok] = len(word2idx)\n\nidx2word   = {i: w for w, i in word2idx.items()}\nvocab_size = len(word2idx)\nprint(\"Vocab size:\", vocab_size)\n# Load GloVe\ndef load_glove(path, dim=100):\n    vectors = {}\n    with open(path, \"r\", encoding=\"utf8\") as fh:\n        for line in fh:\n            parts = line.rstrip().split(\" \")\n            if len(parts) != dim+1:\n                continue\n            word = parts[0]\n            if word in word2idx:            # only store needed words\n                vectors[word] = np.asarray(parts[1:], dtype=np.float32)\n    return vectors\nglove = load_glove(GLOVE_PATH, EMBED_DIM)\nprint(\"Loaded glove vectors:\", len(glove))\n\n# Create embedding matrix\nemb_matrix = np.random.normal(scale=0.6, size=(vocab_size, EMBED_DIM)).astype(np.float32)\nemb_matrix[word2idx[\"<pad>\"]] = np.zeros(EMBED_DIM)\nfor word, vec in glove.items():\n    emb_matrix[word2idx[word]] = vec\n\nword2idx_frozen = word2idx                 # same object; vocab won’t change\nassert emb_matrix.shape[0] == vocab_size, \"vocab/emb mismatch\"\n\n# ---------------------------------------------------------\n# 4. PyTorch Dataset & DataLoader -------------------------\nclass TweetDataset(Dataset):\n    def __init__(self, df, label_available=True):\n        self.tokens = df[\"tokens\"].tolist()\n        self.ids    = df[\"ID\"].tolist()\n        self.label_available = label_available\n        if label_available:\n            self.labels = df[\"Label\"].values\n\n    def __len__(self): return len(self.tokens)\n\n    def __getitem__(self, idx):\n        tokens = self.tokens[idx]\n        idxs = [word2idx_frozen.get(t, 1) for t in tokens]  # 1 = <unk>\n        if self.label_available:\n            return torch.tensor(idxs, dtype=torch.long), self.labels[idx]\n        else:\n            return torch.tensor(idxs, dtype=torch.long), self.ids[idx]\n\ndef collate_batch(batch):\n    token_lists, targets = zip(*batch)\n    offsets = [0]\n    flat = []\n    for tl in token_lists:\n        flat.extend(tl)\n        offsets.append(offsets[-1] + len(tl))\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    flat = torch.tensor(flat, dtype=torch.long)\n    if isinstance(targets[0], (int, np.integer)):\n        targets = torch.tensor(targets, dtype=torch.long)\n        return flat, offsets, targets\n    else:                                 # test set, IDs\n        ids = list(targets)\n        return flat, offsets, ids\n\ntrain_ds = TweetDataset(train_df)\nval_ds   = TweetDataset(val_df)\ntest_ds  = TweetDataset(test_df, label_available=False)\n\nBATCH = 64\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n                          collate_fn=collate_batch, drop_last=False)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,\n                          collate_fn=collate_batch)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False,\n                          collate_fn=collate_batch)\n\n# ---------------------------------------------------------\n# 5. Model ------------------------------------------------\nclass FFNN(nn.Module):\n    def __init__(self, embedding_weight, hidden=128, p_drop=0.5):\n        super().__init__()\n        num_embeddings, emb_dim = embedding_weight.shape\n        self.embed = nn.EmbeddingBag(num_embeddings,\n                                     emb_dim,\n                                     mode='mean',\n                                     _weight=torch.tensor(embedding_weight))\n        self.dropout = nn.Dropout(p_drop)\n        self.fc1 = nn.Linear(emb_dim, hidden)\n        self.fc2 = nn.Linear(hidden, 2)\n\n    def forward(self, tokens, offsets):\n        x = self.embed(tokens, offsets)       # (batch, emb_dim)\n        x = self.dropout(x)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\nmodel = FFNN(emb_matrix, hidden=128, p_drop=0.5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n# ---------------------------------------------------------\n# 6. Train ------------------------------------------------\ndef run_epoch(loader, train=True):\n    epoch_loss, correct, total = 0.0, 0, 0\n    if train: model.train()\n    else:     model.eval()\n    for tokens, offsets, targets in loader:\n        tokens, offsets, targets = (t.to(device) for t in (tokens, offsets, targets))\n        with torch.set_grad_enabled(train):\n            outputs = model(tokens, offsets)\n            loss = criterion(outputs, targets)\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n                optimizer.step()\n        epoch_loss += loss.item() * targets.size(0)\n        preds = outputs.argmax(1)\n        correct += (preds == targets).sum().item()\n        total += targets.size(0)\n    acc  = correct / total\n    loss = epoch_loss / total\n    return loss, acc\n\nbest_val_acc = -1\npatience, patience_left = 2, 2\ntrain_hist, val_hist = [], []\nNUM_EPOCHS = 10\nfor epoch in range(1, NUM_EPOCHS+1):\n    t0 = time.time()\n    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n    va_loss, va_acc = run_epoch(val_loader,   train=False)\n    train_hist.append(tr_acc); val_hist.append(va_acc)\n    print(f\"Epoch {epoch:02d}: train_acc={tr_acc:=.4f} val_acc={va_acc:=.4f} time={time.time()-t0:.1f}s\")\n    if va_acc > best_val_acc:\n        best_val_acc = va_acc\n        torch.save(model.state_dict(), \"best_model.pt\")\n        patience_left = patience\n        print(\"  --> New best, model saved.\")\n    else:\n        patience_left -= 1\n        if patience_left == 0:\n            print(\"Early stopping.\")\n            break\n\n# Load best\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\n\n# ---------------------------------------------------------\n# 7. Evaluation (confusion, ROC, classification report) ---\nmodel.eval()\nall_preds, all_probs, all_targets = [], [], []\n\nwith torch.no_grad():                                 # NEW wrapper\n    for tokens, offsets, targets in val_loader:\n        tokens, offsets = tokens.to(device), offsets.to(device)\n        outputs = model(tokens, offsets)\n        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n        preds = outputs.argmax(1).cpu().numpy()\n        all_preds.extend(preds)\n        all_probs.extend(probs)\n        all_targets.extend(targets.numpy())\nprint(\"\\nClassification report (validation):\")\nprint(classification_report(all_targets, all_preds, target_names=[\"neg\",\"pos\"]))\nprint(\"Accuracy:\", accuracy_score(all_targets, all_preds))\n\n# Confusion matrix\ncm = confusion_matrix(all_targets, all_preds)\nplt.figure(figsize=(4,3))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"neg\",\"pos\"], yticklabels=[\"neg\",\"pos\"])\nplt.ylabel(\"Actual\"); plt.xlabel(\"Predicted\")\nplt.title(\"Confusion Matrix - Validation\")\nplt.tight_layout()\nplt.savefig(\"figures/conf_matrix.png\", dpi=300)\nplt.show()\n\n# ROC\nfpr, tpr, _ = roc_curve(all_targets, all_probs)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, label=f\"AUC={roc_auc:.2f}\")\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - Validation\"); plt.legend()\nplt.tight_layout()\nplt.savefig(\"figures/ROC.png\", dpi=300)\nplt.show()\n\n# Learning curve (training vs val accuracy per epoch)\nepochs = np.arange(1,len(train_hist)+1)\nplt.figure()\nplt.plot(epochs, train_hist, 'o-', label=\"train\")\nplt.plot(epochs, val_hist, 'o-', label=\"val\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\nplt.title(\"Learning Curve\"); plt.legend()\nplt.tight_layout()\nplt.savefig(\"figures/LC.png\", dpi=300)\nplt.show()\n\n# ---------------------------------------------------------\n# 8. Predict on test and create submission ----------------\nmodel.eval()\ntest_labels = []\ntest_ids = []\nwith torch.no_grad():\n    for tokens, offsets, ids in test_loader:\n        tokens, offsets = tokens.to(device), offsets.to(device)\n        preds = model(tokens, offsets).argmax(1).cpu().numpy()\n\n      \n        ids = [int(i) for i in ids]\n\n        test_ids.extend(ids)\n        test_labels.extend(preds)\n\n\n\nprint(\"\\nGenerating appendix figures …\")\n\n# ---------- A1. Word-cloud of the full training corpus ----------\ntry:\n    from wordcloud import WordCloud\nexcept ImportError:\n    # Install wordcloud only if missing (Kaggle lets you pip-install)\n    import subprocess, sys\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"wordcloud\"])\n    from wordcloud import WordCloud\n\ntext_blob = \" \".join(train_df[\"Text\"].astype(str).tolist())\nwc = WordCloud(width=1600, height=800, background_color=\"white\",\n               collocations=False, max_words=300).generate(text_blob)\nwc.to_file(\"figures/wordcloud_all.png\")\nprint(\"   ✓ figures/wordcloud_all.png\")\n\n# ---------- A2. Top-25 token frequency barplot ----------\ntoken_counter = Counter(tok for toks in train_df[\"tokens\"] for tok in toks)\ntop25 = token_counter.most_common(25)\nwords, freqs = zip(*top25)\n\nplt.figure(figsize=(6,6))\nplt.barh(range(len(words))[::-1], freqs[::-1])\nplt.yticks(range(len(words))[::-1], words[::-1])\nplt.xlabel(\"Frequency\"); plt.title(\"Top-25 tokens (training set)\")\nplt.tight_layout()\nplt.savefig(\"figures/token_freq_top25.png\", dpi=300)\nplt.close()\nprint(\"   ✓ figures/token_freq_top25.png\")\n\n# ---------- B. Hyper-parameter heatmap ----------\n\nhp_trials = []  \n\nif hp_trials:\n    hp_df = pd.DataFrame(hp_trials)\n    pivot = hp_df.pivot_table(values=\"val_acc\",\n                              index=\"hidden\", columns=\"dropout\", aggfunc=\"max\")\n    plt.figure(figsize=(6,4))\n    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n    plt.title(\"Validation accuracy by hidden size × dropout\")\n    plt.tight_layout()\n    plt.savefig(\"figures/hp_heatmap.png\", dpi=300)\n    plt.close()\n    print(\"   ✓ figures/hp_heatmap.png\")\nelse:\n    print(\"   (skipped hp_heatmap — hp_trials list is empty)\")\n\n# ---------- C. Histogram of predicted probabilities ----------\nplt.figure(figsize=(6,4))\nplt.hist(all_probs, bins=30, alpha=0.9, edgecolor=\"k\")\nplt.xlabel(\"Predicted P(positive)\"); plt.ylabel(\"Tweet count\")\nplt.title(\"Probability distribution – validation set\")\nplt.tight_layout()\nplt.savefig(\"figures/prob_dist_best.png\", dpi=300)\nplt.close()\nprint(\"   ✓ figures/prob_dist_best.png\")\n\nprint(\"All appendix images saved to figures/\")\n\n\nsubmission = pd.DataFrame({\"ID\": test_ids, \"Label\": test_labels})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"\\nCreated 'submission.csv' with\", submission.shape[0], \"rows\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:22:05.930744Z","iopub.execute_input":"2025-05-01T15:22:05.930955Z","iopub.status.idle":"2025-05-01T15:25:52.24328Z","shell.execute_reply.started":"2025-05-01T15:22:05.930937Z","shell.execute_reply":"2025-05-01T15:25:52.242542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nprint(os.listdir(\"/kaggle/input/glove-twitter\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T12:13:53.179082Z","iopub.execute_input":"2025-04-21T12:13:53.180128Z","iopub.status.idle":"2025-04-21T12:13:53.186768Z","shell.execute_reply.started":"2025-04-21T12:13:53.180098Z","shell.execute_reply":"2025-04-21T12:13:53.185929Z"}},"outputs":[],"execution_count":null}]}