{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98732,"databundleVersionId":11783285,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# DistilBERT fine-tuning â€“ HW-3 Twitter Sentiment\n# ------------------------------------------------\nimport os, random, math, glob, pathlib, warnings, numpy as np, pandas as pd, torch\nimport matplotlib.pyplot as plt, seaborn as sns\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n                             confusion_matrix)\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMODEL_NAME  = \"distilbert-base-uncased\"\nFIG_DIR     = \"figures\"; pathlib.Path(FIG_DIR).mkdir(exist_ok=True)\n\nSEED        = 42\nMAX_LEN     = 128\nBATCH       = 32\nEPOCHS      = 4\nLR          = 2e-5\nWEIGHT_DEC  = 0.01\nPATIENCE    = 2          # early-stopping\n\nID_COL, TEXT_COL, LABEL_COL = \"ID\", \"Text\", \"Label\"\n\ndef seed_everything(seed=SEED):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\nseed_everything()\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locate dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef find_data_root():\n    \"\"\"Return the folder that contains train/val/test CSVs.\"\"\"\n    for path in glob.glob(\"/kaggle/input/*\"):\n        if all(os.path.isfile(os.path.join(path, f\"{split}_dataset.csv\"))\n               for split in (\"train\", \"val\", \"test\")):\n            return path\n    raise FileNotFoundError(\n        \"âŒ CSV files not found. Attach the competition dataset via \"\n        \"â€˜Add â†’ Competitionâ€™ on the right.\"\n    )\n\nDATA_ROOT = find_data_root()\nprint(\"Using data from:\", DATA_ROOT)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ load splits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nraw = load_dataset(\n    \"csv\",\n    data_files={\n        \"train\":      f\"{DATA_ROOT}/train_dataset.csv\",\n        \"validation\": f\"{DATA_ROOT}/val_dataset.csv\",\n        \"test\":       f\"{DATA_ROOT}/test_dataset.csv\",\n    }\n)\nds = DatasetDict(train=raw[\"train\"], validation=raw[\"validation\"], test=raw[\"test\"])\ntest_ids = ds[\"test\"][ID_COL]\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tokenisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntok = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n\ndef tokenize(batch):\n    enc = tok(batch[TEXT_COL], truncation=True, max_length=MAX_LEN)\n    if LABEL_COL in batch and batch[LABEL_COL][0] is not None:\n        enc[\"labels\"] = [int(x) for x in batch[LABEL_COL]]\n    return enc\n\nds_tok = ds.map(tokenize, batched=True,\n                remove_columns=[ID_COL, TEXT_COL, LABEL_COL])\ndata_collator = DataCollatorWithPadding(tok)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef compute_metrics(pred):\n    logits, labels = pred\n    preds = logits.argmax(-1)\n    acc  = accuracy_score(labels, preds)\n    p, r, f1, _ = precision_recall_fscore_support(labels, preds,\n                                                  average=\"macro\", zero_division=0)\n    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ model & trainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n\nargs = TrainingArguments(\n    output_dir                 = \"./checkpoints\",\n    num_train_epochs           = EPOCHS,\n    per_device_train_batch_size= BATCH,\n    per_device_eval_batch_size = BATCH,\n    learning_rate              = LR,\n    weight_decay               = WEIGHT_DEC,\n    eval_strategy              = \"epoch\",\n    save_strategy              = \"epoch\",\n    save_total_limit           = 2,\n    load_best_model_at_end     = True,\n    metric_for_best_model      = \"f1\",\n    seed                       = SEED,\n    fp16                       = torch.cuda.is_available(),\n    report_to                  = \"none\",\n    dataloader_num_workers     = 2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset   = ds_tok[\"train\"],\n    eval_dataset    = ds_tok[\"validation\"],\n    tokenizer       = tok,\n    data_collator   = data_collator,\n    compute_metrics = compute_metrics,\n    callbacks       = [EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrainer.train()\nprint(\"ğŸ“Š  Best validation metrics:\", trainer.evaluate())\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ curves & CM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nhistory = trainer.state.log_history\ntrain_loss = [x[\"loss\"] for x in history if \"loss\" in x and \"epoch\" in x]\neval_f1    = [x[\"eval_f1\"] for x in history if \"eval_f1\" in x]\neval_acc   = [x[\"eval_accuracy\"] for x in history if \"eval_accuracy\" in x]\n\nplt.figure()\nplt.plot(train_loss); plt.xlabel(\"Training step\"); plt.ylabel(\"Loss\")\nplt.title(\"DistilBERT â€“ training loss\"); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/distilbert_loss.png\", dpi=300); plt.close()\n\nplt.figure()\nplt.plot(eval_f1, 'o-', label=\"F1\"); plt.plot(eval_acc, 'o-', label=\"Acc\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.legend()\nplt.title(\"DistilBERT â€“ validation metrics\"); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/distilbert_val_metrics.png\", dpi=300); plt.close()\n\nval_logits = trainer.predict(ds_tok[\"validation\"]).predictions\nval_preds  = val_logits.argmax(-1)\ncm = confusion_matrix(ds[\"validation\"][LABEL_COL], val_preds)\nplt.figure(figsize=(3.5,3))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"neg\",\"pos\"], yticklabels=[\"neg\",\"pos\"])\nplt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\nplt.title(\"DistilBERT â€“ validation CM\"); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/distilbert_confusion.png\", dpi=300); plt.close()\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ test inference & submission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntest_preds = trainer.predict(ds_tok[\"test\"]).predictions.argmax(-1)\nsubmission = pd.DataFrame({ID_COL: test_ids, LABEL_COL: test_preds.astype(int)})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(f\"\\nâœ… submission.csv created with {len(submission)} rows\")\n\n# save best checkpoint\ntrainer.save_model(\"best_checkpoint\"); tok.save_pretrained(\"best_checkpoint\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T16:00:04.781963Z","iopub.execute_input":"2025-05-14T16:00:04.782158Z","iopub.status.idle":"2025-05-14T16:29:46.257528Z","shell.execute_reply.started":"2025-05-14T16:00:04.782141Z","shell.execute_reply":"2025-05-14T16:29:46.256472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show clickable download links for everything inside the `figures/` folder\nfrom IPython.display import FileLinks\n\nFileLinks('figures')   # each filename becomes a hyperlink\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T16:36:28.19134Z","iopub.execute_input":"2025-05-14T16:36:28.192026Z","iopub.status.idle":"2025-05-14T16:36:28.198109Z","shell.execute_reply.started":"2025-05-14T16:36:28.191993Z","shell.execute_reply":"2025-05-14T16:36:28.197395Z"}},"outputs":[],"execution_count":null}]}