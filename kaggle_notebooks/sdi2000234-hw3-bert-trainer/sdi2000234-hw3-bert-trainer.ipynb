{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98723,"databundleVersionId":11781620,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ----------------------------------------------------------------------\n# BERT fine-tuning + plotting (training loss, val metrics, confusion-matrix)\n# ----------------------------------------------------------------------\nimport os, random, numpy as np, pandas as pd, torch, pathlib, matplotlib.pyplot as plt, seaborn as sns\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, DataCollatorWithPadding\n)\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n                             confusion_matrix)\n\nprint(\"Transformers\", __import__(\"transformers\").__version__)\n\n# â”€â”€â”€ configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMODEL_NAME  = \"bert-base-uncased\"\nDATA_ROOT   = \"/kaggle/input/ai-2-dl-for-nlp-2025-homework-3\"  # CSV folder\nMAX_LENGTH  = 128\nBATCH_SIZE  = 16\nNUM_EPOCHS  = 3\nLEARNING_RATE = 2e-5\nSEED          = 42\n\nOUTPUT_DIR = \"/kaggle/working/bert_model\"\nFIG_DIR    = \"/kaggle/working/figures\"\npathlib.Path(OUTPUT_DIR).mkdir(exist_ok=True)\npathlib.Path(FIG_DIR).mkdir(exist_ok=True)\n\nID_COL, TEXT_COL, LABEL_COL = \"ID\", \"Text\", \"Label\"\n\ndef seed_everything(seed=SEED):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nseed_everything()\n\n# â”€â”€â”€ load CSV splits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nraw = load_dataset(\n    \"csv\",\n    data_files={\n        \"train\":      f\"{DATA_ROOT}/train_dataset.csv\",\n        \"validation\": f\"{DATA_ROOT}/val_dataset.csv\",\n        \"test\":       f\"{DATA_ROOT}/test_dataset.csv\",\n    },\n)\nds = DatasetDict(train=raw[\"train\"], validation=raw[\"validation\"], test=raw[\"test\"])\n\n# â”€â”€â”€ tokenisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntok = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n\ndef tokenize(batch):\n    enc = tok(batch[TEXT_COL], truncation=True, max_length=MAX_LENGTH)\n    if LABEL_COL in batch and batch[LABEL_COL][0] is not None:          # train/val splits\n        enc[\"labels\"] = [int(x) for x in batch[LABEL_COL]]\n    return enc\n\nds_tok = ds.map(tokenize, batched=True, remove_columns=[ID_COL, TEXT_COL, LABEL_COL])\ndata_collator = DataCollatorWithPadding(tok)\n\n# â”€â”€â”€ metrics for Trainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef compute_metrics(pred):\n    logits, labels = pred\n    preds = logits.argmax(-1)\n    acc  = accuracy_score(labels, preds)\n    p, r, f1, _ = precision_recall_fscore_support(labels, preds,\n                                                  average=\"macro\", zero_division=0)\n    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n\n# â”€â”€â”€ model + Trainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n\ntraining_args = TrainingArguments(\n    output_dir                    = OUTPUT_DIR,\n    num_train_epochs              = NUM_EPOCHS,\n    per_device_train_batch_size   = BATCH_SIZE,\n    per_device_eval_batch_size    = BATCH_SIZE,\n    learning_rate                 = LEARNING_RATE,\n    eval_strategy                      = \"epoch\",\n    save_strategy                 = \"epoch\",\n    load_best_model_at_end        = True,\n    metric_for_best_model         = \"f1\",\n    seed                          = SEED,\n    report_to                     = \"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset   = ds_tok[\"train\"],\n    eval_dataset    = ds_tok[\"validation\"],\n    tokenizer       = tok,\n    data_collator   = data_collator,\n    compute_metrics = compute_metrics,\n)\n\n# â”€â”€â”€ training & evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrainer.train()\nprint(\"ðŸ“Š  Validation metrics:\", trainer.evaluate())\n\n# â”€â”€â”€ curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nhistory = trainer.state.log_history\ntrain_loss = [x[\"loss\"] for x in history if \"loss\" in x and \"epoch\" in x]\neval_f1    = [x[\"eval_f1\"] for x in history if \"eval_f1\" in x]\neval_acc   = [x[\"eval_accuracy\"] for x in history if \"eval_accuracy\" in x]\n\nplt.figure()\nplt.plot(train_loss)\nplt.xlabel(\"Training step\"); plt.ylabel(\"Loss\")\nplt.title(\"BERT â€“ training loss\"); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/bert_loss.png\", dpi=300); plt.close()\n\nplt.figure()\nplt.plot(eval_f1,  'o-', label=\"F1\")\nplt.plot(eval_acc, 'o-', label=\"Accuracy\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.legend()\nplt.title(\"BERT â€“ validation metrics\"); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/bert_val_metrics.png\", dpi=300); plt.close()\n\n# â”€â”€â”€ confusion-matrix on validation split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nval_logits = trainer.predict(ds_tok[\"validation\"]).predictions\nval_preds  = val_logits.argmax(-1)\ncm = confusion_matrix(ds[\"validation\"][LABEL_COL], val_preds)\n\nplt.figure(figsize=(3.5,3))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"neg\",\"pos\"], yticklabels=[\"neg\",\"pos\"])\nplt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\nplt.title(\"BERT â€“ validation CM\"); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/bert_confusion.png\", dpi=300); plt.close()\n\n# â”€â”€â”€ prediction & submission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntest_preds = trainer.predict(ds_tok[\"test\"]).predictions.argmax(-1)\ntest_ids   = raw[\"test\"][ID_COL]\nsubmission = pd.DataFrame({ID_COL: test_ids, LABEL_COL: test_preds.astype(int)})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv written with\", len(submission), \"rows\")\n\ntrainer.save_model(f\"{OUTPUT_DIR}/best\"); tok.save_pretrained(f\"{OUTPUT_DIR}/best\")\n\nprint(\"Files in /kaggle/working/figures:\", os.listdir(FIG_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:46:01.788616Z","iopub.execute_input":"2025-05-15T10:46:01.7889Z","iopub.status.idle":"2025-05-15T11:36:25.054626Z","shell.execute_reply.started":"2025-05-15T10:46:01.78888Z","shell.execute_reply":"2025-05-15T11:36:25.053761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('figures')          # clickable download links\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:42:06.58687Z","iopub.execute_input":"2025-05-15T11:42:06.587723Z","iopub.status.idle":"2025-05-15T11:42:06.59289Z","shell.execute_reply.started":"2025-05-15T11:42:06.587686Z","shell.execute_reply":"2025-05-15T11:42:06.592199Z"}},"outputs":[],"execution_count":null}]}