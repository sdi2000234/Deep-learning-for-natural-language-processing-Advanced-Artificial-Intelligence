{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92791,"databundleVersionId":11083833,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, confusion_matrix,\n    roc_curve, auc\n)\n\n\n\n# 1) LOAD DATA\ntrain_df = pd.read_csv('/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv')\nval_df   = pd.read_csv('/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/val_dataset.csv')\ntest_df  = pd.read_csv('/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/test_dataset.csv')\n\n# 2) ADVANCED PREPROCESSING (PORTER STEMMER + NEGATION)\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\nnegation_words = {\"no\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"nor\", \"without\"}\nstop_words = stop_words.difference(negation_words)\n\ndef advanced_preprocess(text):\n    \"\"\"\n    1) Removes URLs/@mentions\n    2) Lowercases text\n    3) Normalizes elongated words\n    4) Handles negations\n    5) Removes stopwords (except negations)\n    6) Applies PorterStemmer\n    \"\"\"\n    # Remove URLs, @mentions\n    text = re.sub(r\"http\\S+|www\\S+|@\\S+\", \" \", text)\n    # Remove '#' but keep its text\n    text = text.replace(\"#\", \" \")\n    # Lowercase\n    text = text.lower()\n    # Remove repeated chars\n    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n\n    # Tokenize\n    tokens = word_tokenize(text)\n\n    # Negation handling\n    processed_tokens = []\n    negate = False\n    for token in tokens:\n        if token in negation_words:\n            negate = True\n            processed_tokens.append(token)  # keep negation word\n            continue\n        if token in [\".\", \",\", \"!\", \"?\", \";\", \":\"]:\n            negate = False\n        if negate:\n            token = token + \"_NEG\"\n        processed_tokens.append(token)\n\n    # Filter & Stem\n    filtered_tokens = []\n    for tok in processed_tokens:\n        if tok.isalpha() or \"_NEG\" in tok:  # keep if letter-based or has negation\n            if tok not in stop_words:\n                stemmed = stemmer.stem(tok)\n                filtered_tokens.append(stemmed)\n\n    return \" \".join(filtered_tokens).strip()\n\n# Apply preprocessing\ntrain_df['cleaned_text'] = train_df['Text'].apply(advanced_preprocess)\nval_df['cleaned_text']   = val_df['Text'].apply(advanced_preprocess)\ntest_df['cleaned_text']  = test_df['Text'].apply(advanced_preprocess)\n\n# 3) TF-IDF VECTORIZATION\nvectorizer = TfidfVectorizer(\n    ngram_range=(1, 2),  # unigrams + bigrams\n    min_df=2,            # drop words appearing once\n    sublinear_tf=True    # log-scaling term frequency\n)\nX_train = vectorizer.fit_transform(train_df['cleaned_text'])\ny_train = train_df['Label']\n\nX_val = vectorizer.transform(val_df['cleaned_text'])\ny_val = val_df['Label']\n\nX_test = vectorizer.transform(test_df['cleaned_text'])\n\n# 4) LOGISTIC REGRESSION WITH TUNED HYPERPARAMS\nmodel = LogisticRegression(\n    penalty='l2',\n    C=0.5,\n    solver='liblinear',\n    max_iter=200,\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n\n# 5) VALIDATION METRICS\ny_val_pred = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nprint(\"\\n=== Validation Performance ===\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n\n# 5.1) Confusion Matrix\ncm = confusion_matrix(y_val, y_val_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix (Validation)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# 5.2) ROC Curve\n# For ROC, we need predicted probabilities for class 1\ny_val_probs = model.predict_proba(X_val)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_val, y_val_probs)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", lw=2)\nplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"r\", label=\"Random Guess\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - Validation\")\nplt.legend(loc=\"lower right\")\nplt.show()\nprint(f\"Validation AUC: {roc_auc:.4f}\")\n\n# 5.3) LEARNING CURVE\n# Evaluate how model performance changes as we vary the training set size.\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    estimator=model,\n    X=X_train,\n    y=y_train,\n    cv=5,            # 5-fold CV\n    scoring='accuracy',\n    train_sizes=np.linspace(0.1, 1.0, 5),\n    random_state=42\n)\n\ntrain_means = np.mean(train_scores, axis=1)\nval_means   = np.mean(val_scores, axis=1)\n\nplt.figure()\nplt.plot(train_sizes, train_means, 'o-', label='Training Accuracy')\nplt.plot(train_sizes, val_means, 'o-', label='Validation Accuracy')\nplt.title(\"Learning Curve (Logistic Regression)\")\nplt.xlabel(\"Training Examples\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n# 6) FINAL SUBMISSION\ntest_preds = model.predict(X_test)\nsubmission_df = pd.DataFrame({\n    'ID': test_df['ID'],\n    'Label': test_preds\n})\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"\\n=> 'submission.csv' created for Kaggle.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download(\"punkt\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:49:15.958767Z","iopub.execute_input":"2025-02-24T01:49:15.959107Z","iopub.status.idle":"2025-02-24T01:49:16.480662Z","shell.execute_reply.started":"2025-02-24T01:49:15.959073Z","shell.execute_reply":"2025-02-24T01:49:16.47953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T20:08:47.644582Z","iopub.execute_input":"2025-03-01T20:08:47.645122Z","iopub.status.idle":"2025-03-01T20:08:47.789213Z","shell.execute_reply.started":"2025-03-01T20:08:47.645076Z","shell.execute_reply":"2025-03-01T20:08:47.787949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}